{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, GPT2TokenizerFast, BertTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = './data_ezsocket/merged_api_para_pcapdata_dataset20k.csv'\n",
    "# ; is the tab character in Python\n",
    "ezsocket_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "####以下函数用于讲10进制数token化#####\n",
    "def format_decimal_as_hexadecimal(decimal_str):\n",
    "    # Convert the decimal string to an integer\n",
    "    decimal_number = int(decimal_str)\n",
    "    \n",
    "    # Convert the integer to a hexadecimal string\n",
    "    hex_str = hex(decimal_number)[2:]  # Strip the '0x' prefix\n",
    "    \n",
    "    # Ensure the length of the hex string is even\n",
    "    if len(hex_str) % 2 != 0:\n",
    "        hex_str = '0' + hex_str\n",
    "    \n",
    "    # Split the hex string into pairs of characters\n",
    "    hex_pairs = [hex_str[i:i+2] for i in range(0, len(hex_str), 2)]\n",
    "    \n",
    "    # Join the pairs with commas\n",
    "    formatted_hex = ','.join(hex_pairs)\n",
    "    \n",
    "    return formatted_hex\n",
    "\n",
    "\n",
    "def convert_number(num_str):\n",
    "    num = float(num_str)\n",
    "    if num.is_integer():\n",
    "        num = int(num)\n",
    "        sign = \"-\" if num < 0 else \"+\"\n",
    "        num_str = format_decimal_as_hexadecimal(str(num).lstrip('-'))\n",
    "        return f\"num,{sign},{num_str},num\"\n",
    "    else:\n",
    "        sign = \"-\" if num < 0 else \"+\"\n",
    "        num_str = num_str.lstrip('-')\n",
    "        integer_part, fractional_part = num_str.split('.')\n",
    "        combined_num = format_decimal_as_hexadecimal(integer_part + fractional_part.rstrip('0'))\n",
    "        pos_num = format_decimal_as_hexadecimal(len(fractional_part.rstrip('0')))\n",
    "        return f\"num,{sign},{combined_num},pos,{pos_num},num\"\n",
    "\n",
    "def process_segment(segment):\n",
    "    parts = segment.split(',')\n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(r'^-?\\d+(\\.\\d+)?$', part):  # Match integers and floating-point numbers\n",
    "            parts[i] = convert_number(part)\n",
    "    result = ','.join(parts)\n",
    "    result = result.replace(\",\", \" \")\n",
    "    return result\n",
    "####以上函数用于将10进制数token化：process_segment(segment)#####\n",
    "\n",
    "####以下函数用于将payload按两位分开，用','隔开#####\n",
    "def split_payload_into_pairs(text):\n",
    "    # 将文本按每两个字符分割\n",
    "    pairs = [text[i:i+2] for i in range(0, len(text), 2)]\n",
    "    # 用逗号连接分割后的文本\n",
    "    result = ' '.join(pairs)\n",
    "    return result\n",
    "####以上函数用于将payload按两位分开，用','隔开#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2315c2c00f4002907a85c29d5b7dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb9c3ebe5d9470c8cd63dc72198c3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22c7dc943be4f17b3921f9adfc11fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20311 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#使用map+lambda清洗数据\n",
    "# clear_ezsocket_dataset = ezsocket_dataset.map(lambda x: {\"Function and Parameters\": x[\"Function and Parameters\"].split(',', 1)[1]})\n",
    "clear_ezsocket_dataset = ezsocket_dataset.map(lambda x: {\"Function and Parameters\": [o.split(',', 1)[1] for o in x[\"Function and Parameters\"]]}, batched=True) #可加速处理，删除前面的时间戳\n",
    "clear_ezsocket_dataset = clear_ezsocket_dataset.map(lambda x: {\"Function and Parameters\": [process_segment(o) for o in x[\"Function and Parameters\"]]}, batched=True) #可加速处理，10进制参数token化\n",
    "clear_ezsocket_dataset = clear_ezsocket_dataset.map(lambda x: {\"Data Segment\": [split_payload_into_pairs(o) for o in x[\"Data Segment\"]]}, batched=True) #可加速处理，10进制参数token化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 14623\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 1625\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 4063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#划分训练集测试集和验证集\n",
    "ezsocket_dataset_tt = clear_ezsocket_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "ezsocket_dataset_tvt = ezsocket_dataset_tt[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "ezsocket_dataset_tvt[\"validation\"] = ezsocket_dataset_tvt.pop(\"test\")\n",
    "ezsocket_dataset_tvt[\"test\"] = ezsocket_dataset_tt[\"test\"]\n",
    "ezsocket_dataset_tvt\n",
    "#保存数据集使用：Arrow:\tDataset.save_to_disk()  CSV:\tDataset.to_csv()    JSON:\tDataset.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport json\\n\\n#手动tokenizer,直接生成词汇表,gpt2的词汇表是json格式 \\nword_list = set([])\\nfor line in ezsocket_dataset_tvt[\"train\"][\"Function and Parameters\"]:\\n    line = line.split()\\n    word_list.update(set(line)) # [\\'hello\\', \\'how\\', \\'are\\', \\'you\\',...]\\nfiltered_list = [item.lower() for item in word_list if len(item) >= 6]\\n\\nword2idx = {f\"{i:02X}\".lower(): i for i in range(256)}\\nword2idx.update({\\'[MASK]\\' : 256, \\'[CLS]\\' : 257, \\'[SEP]\\' : 258, \\'num\\' : 259, \\'pos\\' : 260, \\'[PAD]\\' : 261, \\'+\\' : 262, \\'-\\' : 263})\\nfor i, w in enumerate(filtered_list):\\n    word2idx[w] = i + 264\\n\\njson_str = json.dumps(word2idx)\\nwith open(\\'vocab.json\\', \\'w\\') as json_file:\\n    json_file.write(json_str)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import json\n",
    "\n",
    "#手动tokenizer,直接生成词汇表,gpt2的词汇表是json格式 \n",
    "word_list = set([])\n",
    "for line in ezsocket_dataset_tvt[\"train\"][\"Function and Parameters\"]:\n",
    "    line = line.split()\n",
    "    word_list.update(set(line)) # ['hello', 'how', 'are', 'you',...]\n",
    "filtered_list = [item.lower() for item in word_list if len(item) >= 6]\n",
    "\n",
    "word2idx = {f\"{i:02X}\".lower(): i for i in range(256)}\n",
    "word2idx.update({'[MASK]' : 256, '[CLS]' : 257, '[SEP]' : 258, 'num' : 259, 'pos' : 260, '[PAD]' : 261, '+' : 262, '-' : 263})\n",
    "for i, w in enumerate(filtered_list):\n",
    "    word2idx[w] = i + 264\n",
    "\n",
    "json_str = json.dumps(word2idx)\n",
    "with open('vocab.json', 'w') as json_file:\n",
    "    json_file.write(json_str)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = GPT2TokenizerFast(vocab_file=\"./vocab.json\", merges_file=\"./merges.txt\")\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer=BertTokenizer(vocab_file='./vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocabulary size: 358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[257, 357, 258]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print tokenizer vocabulary size\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "tokenizer.special_tokens_map\n",
    "tokenizer.encode('[UNK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('00', 0),\n",
       "             ('01', 1),\n",
       "             ('02', 2),\n",
       "             ('03', 3),\n",
       "             ('04', 4),\n",
       "             ('05', 5),\n",
       "             ('06', 6),\n",
       "             ('07', 7),\n",
       "             ('08', 8),\n",
       "             ('09', 9),\n",
       "             ('0a', 10),\n",
       "             ('0b', 11),\n",
       "             ('0c', 12),\n",
       "             ('0d', 13),\n",
       "             ('0e', 14),\n",
       "             ('0f', 15),\n",
       "             ('10', 16),\n",
       "             ('11', 17),\n",
       "             ('12', 18),\n",
       "             ('13', 19),\n",
       "             ('14', 20),\n",
       "             ('15', 21),\n",
       "             ('16', 22),\n",
       "             ('17', 23),\n",
       "             ('18', 24),\n",
       "             ('19', 25),\n",
       "             ('1a', 26),\n",
       "             ('1b', 27),\n",
       "             ('1c', 28),\n",
       "             ('1d', 29),\n",
       "             ('1e', 30),\n",
       "             ('1f', 31),\n",
       "             ('20', 32),\n",
       "             ('21', 33),\n",
       "             ('22', 34),\n",
       "             ('23', 35),\n",
       "             ('24', 36),\n",
       "             ('25', 37),\n",
       "             ('26', 38),\n",
       "             ('27', 39),\n",
       "             ('28', 40),\n",
       "             ('29', 41),\n",
       "             ('2a', 42),\n",
       "             ('2b', 43),\n",
       "             ('2c', 44),\n",
       "             ('2d', 45),\n",
       "             ('2e', 46),\n",
       "             ('2f', 47),\n",
       "             ('30', 48),\n",
       "             ('31', 49),\n",
       "             ('32', 50),\n",
       "             ('33', 51),\n",
       "             ('34', 52),\n",
       "             ('35', 53),\n",
       "             ('36', 54),\n",
       "             ('37', 55),\n",
       "             ('38', 56),\n",
       "             ('39', 57),\n",
       "             ('3a', 58),\n",
       "             ('3b', 59),\n",
       "             ('3c', 60),\n",
       "             ('3d', 61),\n",
       "             ('3e', 62),\n",
       "             ('3f', 63),\n",
       "             ('40', 64),\n",
       "             ('41', 65),\n",
       "             ('42', 66),\n",
       "             ('43', 67),\n",
       "             ('44', 68),\n",
       "             ('45', 69),\n",
       "             ('46', 70),\n",
       "             ('47', 71),\n",
       "             ('48', 72),\n",
       "             ('49', 73),\n",
       "             ('4a', 74),\n",
       "             ('4b', 75),\n",
       "             ('4c', 76),\n",
       "             ('4d', 77),\n",
       "             ('4e', 78),\n",
       "             ('4f', 79),\n",
       "             ('50', 80),\n",
       "             ('51', 81),\n",
       "             ('52', 82),\n",
       "             ('53', 83),\n",
       "             ('54', 84),\n",
       "             ('55', 85),\n",
       "             ('56', 86),\n",
       "             ('57', 87),\n",
       "             ('58', 88),\n",
       "             ('59', 89),\n",
       "             ('5a', 90),\n",
       "             ('5b', 91),\n",
       "             ('5c', 92),\n",
       "             ('5d', 93),\n",
       "             ('5e', 94),\n",
       "             ('5f', 95),\n",
       "             ('60', 96),\n",
       "             ('61', 97),\n",
       "             ('62', 98),\n",
       "             ('63', 99),\n",
       "             ('64', 100),\n",
       "             ('65', 101),\n",
       "             ('66', 102),\n",
       "             ('67', 103),\n",
       "             ('68', 104),\n",
       "             ('69', 105),\n",
       "             ('6a', 106),\n",
       "             ('6b', 107),\n",
       "             ('6c', 108),\n",
       "             ('6d', 109),\n",
       "             ('6e', 110),\n",
       "             ('6f', 111),\n",
       "             ('70', 112),\n",
       "             ('71', 113),\n",
       "             ('72', 114),\n",
       "             ('73', 115),\n",
       "             ('74', 116),\n",
       "             ('75', 117),\n",
       "             ('76', 118),\n",
       "             ('77', 119),\n",
       "             ('78', 120),\n",
       "             ('79', 121),\n",
       "             ('7a', 122),\n",
       "             ('7b', 123),\n",
       "             ('7c', 124),\n",
       "             ('7d', 125),\n",
       "             ('7e', 126),\n",
       "             ('7f', 127),\n",
       "             ('80', 128),\n",
       "             ('81', 129),\n",
       "             ('82', 130),\n",
       "             ('83', 131),\n",
       "             ('84', 132),\n",
       "             ('85', 133),\n",
       "             ('86', 134),\n",
       "             ('87', 135),\n",
       "             ('88', 136),\n",
       "             ('89', 137),\n",
       "             ('8a', 138),\n",
       "             ('8b', 139),\n",
       "             ('8c', 140),\n",
       "             ('8d', 141),\n",
       "             ('8e', 142),\n",
       "             ('8f', 143),\n",
       "             ('90', 144),\n",
       "             ('91', 145),\n",
       "             ('92', 146),\n",
       "             ('93', 147),\n",
       "             ('94', 148),\n",
       "             ('95', 149),\n",
       "             ('96', 150),\n",
       "             ('97', 151),\n",
       "             ('98', 152),\n",
       "             ('99', 153),\n",
       "             ('9a', 154),\n",
       "             ('9b', 155),\n",
       "             ('9c', 156),\n",
       "             ('9d', 157),\n",
       "             ('9e', 158),\n",
       "             ('9f', 159),\n",
       "             ('a0', 160),\n",
       "             ('a1', 161),\n",
       "             ('a2', 162),\n",
       "             ('a3', 163),\n",
       "             ('a4', 164),\n",
       "             ('a5', 165),\n",
       "             ('a6', 166),\n",
       "             ('a7', 167),\n",
       "             ('a8', 168),\n",
       "             ('a9', 169),\n",
       "             ('aa', 170),\n",
       "             ('ab', 171),\n",
       "             ('ac', 172),\n",
       "             ('ad', 173),\n",
       "             ('ae', 174),\n",
       "             ('af', 175),\n",
       "             ('b0', 176),\n",
       "             ('b1', 177),\n",
       "             ('b2', 178),\n",
       "             ('b3', 179),\n",
       "             ('b4', 180),\n",
       "             ('b5', 181),\n",
       "             ('b6', 182),\n",
       "             ('b7', 183),\n",
       "             ('b8', 184),\n",
       "             ('b9', 185),\n",
       "             ('ba', 186),\n",
       "             ('bb', 187),\n",
       "             ('bc', 188),\n",
       "             ('bd', 189),\n",
       "             ('be', 190),\n",
       "             ('bf', 191),\n",
       "             ('c0', 192),\n",
       "             ('c1', 193),\n",
       "             ('c2', 194),\n",
       "             ('c3', 195),\n",
       "             ('c4', 196),\n",
       "             ('c5', 197),\n",
       "             ('c6', 198),\n",
       "             ('c7', 199),\n",
       "             ('c8', 200),\n",
       "             ('c9', 201),\n",
       "             ('ca', 202),\n",
       "             ('cb', 203),\n",
       "             ('cc', 204),\n",
       "             ('cd', 205),\n",
       "             ('ce', 206),\n",
       "             ('cf', 207),\n",
       "             ('d0', 208),\n",
       "             ('d1', 209),\n",
       "             ('d2', 210),\n",
       "             ('d3', 211),\n",
       "             ('d4', 212),\n",
       "             ('d5', 213),\n",
       "             ('d6', 214),\n",
       "             ('d7', 215),\n",
       "             ('d8', 216),\n",
       "             ('d9', 217),\n",
       "             ('da', 218),\n",
       "             ('db', 219),\n",
       "             ('dc', 220),\n",
       "             ('dd', 221),\n",
       "             ('de', 222),\n",
       "             ('df', 223),\n",
       "             ('e0', 224),\n",
       "             ('e1', 225),\n",
       "             ('e2', 226),\n",
       "             ('e3', 227),\n",
       "             ('e4', 228),\n",
       "             ('e5', 229),\n",
       "             ('e6', 230),\n",
       "             ('e7', 231),\n",
       "             ('e8', 232),\n",
       "             ('e9', 233),\n",
       "             ('ea', 234),\n",
       "             ('eb', 235),\n",
       "             ('ec', 236),\n",
       "             ('ed', 237),\n",
       "             ('ee', 238),\n",
       "             ('ef', 239),\n",
       "             ('f0', 240),\n",
       "             ('f1', 241),\n",
       "             ('f2', 242),\n",
       "             ('f3', 243),\n",
       "             ('f4', 244),\n",
       "             ('f5', 245),\n",
       "             ('f6', 246),\n",
       "             ('f7', 247),\n",
       "             ('f8', 248),\n",
       "             ('f9', 249),\n",
       "             ('fa', 250),\n",
       "             ('fb', 251),\n",
       "             ('fc', 252),\n",
       "             ('fd', 253),\n",
       "             ('fe', 254),\n",
       "             ('ff', 255),\n",
       "             ('[MASK]', 256),\n",
       "             ('[CLS]', 257),\n",
       "             ('[SEP]', 258),\n",
       "             ('num', 259),\n",
       "             ('pos', 260),\n",
       "             ('[PAD]', 261),\n",
       "             ('+', 262),\n",
       "             ('-', 263),\n",
       "             ('getmachineposition', 264),\n",
       "             ('getprogramposition3', 265),\n",
       "             ('getworkinstallationposition', 266),\n",
       "             ('getstarttime', 267),\n",
       "             ('getmgnpot', 268),\n",
       "             ('getservoversion', 269),\n",
       "             ('getauxaxismonitor', 270),\n",
       "             ('getalivetime', 271),\n",
       "             ('gettcpspeed', 272),\n",
       "             ('getalarm', 273),\n",
       "             ('setruntime', 274),\n",
       "             ('gettablecoordinationposition', 275),\n",
       "             ('getspindleversion', 276),\n",
       "             ('getsubprolevel', 277),\n",
       "             ('getsysteminformation', 278),\n",
       "             ('getcommand2', 279),\n",
       "             ('getfeedbackposition', 280),\n",
       "             ('getsize', 281),\n",
       "             ('getspindlediagnosis', 282),\n",
       "             ('gettoolworkoffset', 283),\n",
       "             ('getmgnsize', 284),\n",
       "             ('getprogramposition', 285),\n",
       "             ('gettoolcommand', 286),\n",
       "             ('getmanualoverlap2', 287),\n",
       "             ('getrunstatus', 288),\n",
       "             ('getsequencenumber', 289),\n",
       "             ('getsurface', 290),\n",
       "             ('getdistance', 291),\n",
       "             ('deletetoollifetoolno', 292),\n",
       "             ('setestimatetime', 293),\n",
       "             ('getoffset2', 294),\n",
       "             ('getruntime', 295),\n",
       "             ('getblocknumber', 296),\n",
       "             ('deletetoollifegroup', 297),\n",
       "             ('getmgncontrol', 298),\n",
       "             ('gettcpmachineposition', 299),\n",
       "             ('getmgnaux', 300),\n",
       "             ('getversion', 301),\n",
       "             ('setmgnpot3', 302),\n",
       "             ('gettoolsetsize', 303),\n",
       "             ('getpowerversion', 304),\n",
       "             ('gettoolworkoffset2', 305),\n",
       "             ('getabspositionmonitor', 306),\n",
       "             ('getdoweltime', 307),\n",
       "             ('getmanualoverlap', 308),\n",
       "             ('setmgnpot', 309),\n",
       "             ('currentblockread', 310),\n",
       "             ('changetoollifetoolno', 311),\n",
       "             ('getaxisstatus', 312),\n",
       "             ('getauxaxisdiagnosis', 313),\n",
       "             ('getinformation', 314),\n",
       "             ('commonvwrite', 315),\n",
       "             ('getsurface2', 316),\n",
       "             ('setalivetime', 317),\n",
       "             ('changetoollifegroup', 318),\n",
       "             ('getservodiagnosis', 319),\n",
       "             ('getgcodecommand', 320),\n",
       "             ('getdistance2', 321),\n",
       "             ('getestimatetime', 322),\n",
       "             ('getprogramnumber2', 323),\n",
       "             ('getoffset', 324),\n",
       "             ('setoffset', 325),\n",
       "             ('getinvalidstatus', 326),\n",
       "             ('getauxaxisversion', 327),\n",
       "             ('commonvread', 328),\n",
       "             ('getname', 329),\n",
       "             ('getmgnsize2', 330),\n",
       "             ('getcuttingmode', 331),\n",
       "             ('getspindlemonitor', 332),\n",
       "             ('getcommandstatus', 333),\n",
       "             ('setsurface', 334),\n",
       "             ('gettype', 335),\n",
       "             ('getclockdata', 336),\n",
       "             ('setcommand2', 337),\n",
       "             ('gettoollifetype2', 338),\n",
       "             ('getcurrentposition', 339),\n",
       "             ('getpowerdiagnosis', 340),\n",
       "             ('setstarttime', 341),\n",
       "             ('getnextdistance', 342),\n",
       "             ('getmgnready2', 343),\n",
       "             ('settoollifetype2', 344),\n",
       "             ('setmgnaux', 345),\n",
       "             ('getworkposition', 346),\n",
       "             ('getservomonitor', 347),\n",
       "             ('getfeedspeed', 348),\n",
       "             ('getfeedcommand', 349),\n",
       "             ('getinclinedsurfaceposition', 350),\n",
       "             ('getmacrolevel', 351),\n",
       "             ('getcurrentblockbybyte', 352),\n",
       "             ('getmgnpot3', 353),\n",
       "             ('getmachineposition2', 354),\n",
       "             ('setclockdata', 355),\n",
       "             ('getworkposition2', 356)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ezsocket_dataset_tvt[\"train\"][\"Function and Parameters\"][0]\n",
    "tokenizer(\"\")\n",
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c0e1ddb2b64cb3a82707e58a2c6bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/14623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee7d973cdb8456e82c6163a6d1ef9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/1625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e53dc403b7c4ec9af989ac675c31269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/4063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Handle potential None values and convert to empty string if None\n",
    "    input_texts = [str(text) if text is not None else \"\" for text in examples[\"Function and Parameters\"]]\n",
    "    output_texts = [str(text) if text is not None else \"\" for text in examples[\"Data Segment\"]]\n",
    "    # print(input_texts)\n",
    "    # print(tokenizer.sep_token)\n",
    "    # Splice the input and output strings\n",
    "    spliced_texts = [input_text +  output_text \n",
    "                     for input_text, output_text in zip(input_texts, output_texts)]\n",
    "    # print(spliced_texts)\n",
    "    # return spliced_texts\n",
    "    # Tokenize the spliced texts\n",
    "    tokenized = tokenizer(spliced_texts, truncation=True, max_length=210)\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = ezsocket_dataset_tvt.map(tokenize_function, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Function and Parameters': 'SetMGNAux num + 95 9c num ',\n",
       " 'Data Segment': '47 49 4f 50 01 00 01 00 4c 00 00 00 00 00 00 00 e0 37 00 00 01 00 00 00 04 00 00 00 01 00 00 00 0d 00 00 00 6d 6f 63 68 61 53 65 74 44 61 74 61 00 3c 23 77 00 00 00 00 37 00 00 00 0c b0 01 00 00 00 00 00 00 00 00 00 00 00 00 00 03 00 00 00 04 00 00 00 9c 95 00 00',\n",
       " 'input_ids': [257,\n",
       "  345,\n",
       "  259,\n",
       "  262,\n",
       "  149,\n",
       "  156,\n",
       "  259,\n",
       "  71,\n",
       "  73,\n",
       "  79,\n",
       "  80,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  76,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  224,\n",
       "  55,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  13,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  109,\n",
       "  111,\n",
       "  99,\n",
       "  104,\n",
       "  97,\n",
       "  83,\n",
       "  101,\n",
       "  116,\n",
       "  68,\n",
       "  97,\n",
       "  116,\n",
       "  97,\n",
       "  0,\n",
       "  60,\n",
       "  35,\n",
       "  119,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  55,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  12,\n",
       "  176,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  156,\n",
       "  149,\n",
       "  0,\n",
       "  0,\n",
       "  258],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_positions=1024,\n",
    "    n_ctx=1024,\n",
    "    n_embd=768,\n",
    "    n_layer=12,\n",
    "    n_head=12,\n",
    ")\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(357, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=357, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_pretrained\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=128,\n",
    "    per_device_eval_batch_size=128,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    save_total_limit=8,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16005' max='16005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16005/16005 3:34:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.173700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.172100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.167000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.157200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.156600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.157300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.155700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.155000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16005, training_loss=0.16406450695784455, metrics={'train_runtime': 12861.3419, 'train_samples_per_second': 159.27, 'train_steps_per_second': 1.244, 'total_flos': 1.1293563186432e+17, 'train_loss': 0.16406450695784455, 'epoch': 5.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_hug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
