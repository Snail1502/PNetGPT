{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, EncoderDecoderModel, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2944b9e22f9c467bb580ca8146d97251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27069424f05425fb61bbdd1f7add1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91461c2904fc458688d7159bf56853a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:765: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data_files = './data_ezsocket/merged_api_para_pcapdata_dataset100k.csv'\n",
    "# ; is the tab character in Python\n",
    "ezsocket_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GetMGNPot3 num - 02 23 9e d7 pos 07 num num + 61 num \n",
      "47 49 4f 50 01 00 01 00 44 00 00 00 00 00 00 00 e0 37 00 00 01 00 2c 00 04 00 00 00 01 00 00 00 0d 00 00 00 6d 6f 63 68 61 47 65 74 44 61 74 61 00 3c 23 77 00 00 00 00 37 00 00 00 16 b0 01 00 00 00 00 00 00 00 00 00 00 00 00 00 02 00 00 00\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "####ä»¥ä¸‹å‡½æ•°ç”¨äºŽè®²10è¿›åˆ¶æ•°tokenåŒ–#####\n",
    "def format_decimal_as_hexadecimal(decimal_str):\n",
    "    # Convert the decimal string to an integer\n",
    "    decimal_number = int(decimal_str)\n",
    "    \n",
    "    # Convert the integer to a hexadecimal string\n",
    "    hex_str = hex(decimal_number)[2:]  # Strip the '0x' prefix\n",
    "    \n",
    "    # Ensure the length of the hex string is even\n",
    "    if len(hex_str) % 2 != 0:\n",
    "        hex_str = '0' + hex_str\n",
    "    \n",
    "    # Split the hex string into pairs of characters\n",
    "    hex_pairs = [hex_str[i:i+2] for i in range(0, len(hex_str), 2)]\n",
    "    \n",
    "    # Join the pairs with commas\n",
    "    formatted_hex = ','.join(hex_pairs)\n",
    "    \n",
    "    return formatted_hex\n",
    "\n",
    "\n",
    "def convert_number(num_str):\n",
    "    num = float(num_str)\n",
    "    if num.is_integer():\n",
    "        num = int(num)\n",
    "        sign = \"-\" if num < 0 else \"+\"\n",
    "        num_str = format_decimal_as_hexadecimal(str(num).lstrip('-'))\n",
    "        return f\"num,{sign},{num_str},num\"\n",
    "    else:\n",
    "        sign = \"-\" if num < 0 else \"+\"\n",
    "        num_str = num_str.lstrip('-')\n",
    "        integer_part, fractional_part = num_str.split('.')\n",
    "        combined_num = format_decimal_as_hexadecimal(integer_part + fractional_part.rstrip('0'))\n",
    "        pos_num = format_decimal_as_hexadecimal(len(fractional_part.rstrip('0')))\n",
    "        return f\"num,{sign},{combined_num},pos,{pos_num},num\"\n",
    "\n",
    "def process_segment(segment):\n",
    "    parts = segment.split(',')\n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(r'^-?\\d+(\\.\\d+)?$', part):  # Match integers and floating-point numbers\n",
    "            parts[i] = convert_number(part)\n",
    "    result = ','.join(parts)\n",
    "    result = result.replace(\",\", \" \")\n",
    "    return result\n",
    "####ä»¥ä¸Šå‡½æ•°ç”¨äºŽå°†10è¿›åˆ¶æ•°tokenåŒ–ï¼šprocess_segment(segment)#####\n",
    "\n",
    "####ä»¥ä¸‹å‡½æ•°ç”¨äºŽå°†payloadæŒ‰ä¸¤ä½åˆ†å¼€ï¼Œç”¨','éš”å¼€#####\n",
    "def split_payload_into_pairs(text):\n",
    "    # å°†æ–‡æœ¬æŒ‰æ¯ä¸¤ä¸ªå­—ç¬¦åˆ†å‰²\n",
    "    pairs = [text[i:i+2] for i in range(0, len(text), 2)]\n",
    "    # ç”¨é€—å·è¿žæŽ¥åˆ†å‰²åŽçš„æ–‡æœ¬\n",
    "    result = ' '.join(pairs)\n",
    "    return result\n",
    "####ä»¥ä¸Šå‡½æ•°ç”¨äºŽå°†payloadæŒ‰ä¸¤ä½åˆ†å¼€ï¼Œç”¨','éš”å¼€#####\n",
    "\n",
    "print(process_segment('GetMGNPot3,-3.5888855,97,'))\n",
    "print(split_payload_into_pairs('47494f50010001004400000000000000e037000001002c0004000000010000000d0000006d6f63686147657444617461003c2377000000003700000016b0010000000000000000000000000002000000'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613254160096424bb7c114108ade6526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016c346908c845b08300539b1df56d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "692ac06864c94b07a11d36cbf7ab5d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/113871 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ä½¿ç”¨map+lambdaæ¸…æ´—æ•°æ®\n",
    "# clear_ezsocket_dataset = ezsocket_dataset.map(lambda x: {\"Function and Parameters\": x[\"Function and Parameters\"].split(',', 1)[1]})\n",
    "clear_ezsocket_dataset = ezsocket_dataset.map(lambda x: {\"Function and Parameters\": [o.split(',', 1)[1] for o in x[\"Function and Parameters\"]]}, batched=True) #å¯åŠ é€Ÿå¤„ç†ï¼Œåˆ é™¤å‰é¢çš„æ—¶é—´æˆ³\n",
    "clear_ezsocket_dataset = clear_ezsocket_dataset.map(lambda x: {\"Function and Parameters\": [process_segment(o) for o in x[\"Function and Parameters\"]]}, batched=True) #å¯åŠ é€Ÿå¤„ç†ï¼Œ10è¿›åˆ¶å‚æ•°tokenåŒ–\n",
    "clear_ezsocket_dataset = clear_ezsocket_dataset.map(lambda x: {\"Data Segment\": [split_payload_into_pairs(o) for o in x[\"Data Segment\"]]}, batched=True) #å¯åŠ é€Ÿå¤„ç†ï¼Œ10è¿›åˆ¶å‚æ•°tokenåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 81986\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 9110\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 22775\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#åˆ’åˆ†è®­ç»ƒé›†æµ‹è¯•é›†å’ŒéªŒè¯é›†\n",
    "ezsocket_dataset_tt = clear_ezsocket_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "ezsocket_dataset_tvt = ezsocket_dataset_tt[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "ezsocket_dataset_tvt[\"validation\"] = ezsocket_dataset_tvt.pop(\"test\")\n",
    "ezsocket_dataset_tvt[\"test\"] = ezsocket_dataset_tt[\"test\"]\n",
    "ezsocket_dataset_tvt\n",
    "#ä¿å­˜æ•°æ®é›†ä½¿ç”¨ï¼šArrow:\tDataset.save_to_disk()  CSV:\tDataset.to_csv()    JSON:\tDataset.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer(vocab_file='./vocab.txt')\n",
    "src_tokenizer = BertTokenizer(vocab_file='./vocab.txt')\n",
    "tgt_tokenizer = BertTokenizer(vocab_file='./vocab.txt')  # Or your target language tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at ./gpt2_pretrained-500k/checkpoint-16000-500k and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.10.crossattention.c_attn.bias', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.10.crossattention.q_attn.bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.ln_cross_attn.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.11.crossattention.c_attn.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.crossattention.q_attn.bias', 'transformer.h.11.crossattention.q_attn.weight', 'transformer.h.11.ln_cross_attn.bias', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.6.crossattention.c_attn.bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.6.crossattention.q_attn.bias', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.6.ln_cross_attn.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.7.crossattention.c_attn.bias', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.7.crossattention.q_attn.bias', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.7.ln_cross_attn.bias', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.bias', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.bias', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.8.ln_cross_attn.bias', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.9.crossattention.c_attn.bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.9.crossattention.q_attn.bias', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.9.ln_cross_attn.bias', 'transformer.h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pretrained_model = CustomEncoderDecoderModel.from_encoder_decoder_pretrained(\"./bert_pretrained_100k/checkpoint-6400\", \"./bert_pretrained_100k/checkpoint-6400\")\n",
    "# Load your pre-trained model\n",
    "pretrained_model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"./gpt2_pretrained-500k/checkpoint-16000-500k\", \"./gpt2_pretrained-500k/checkpoint-16000-500k\")\n",
    "\n",
    "# Set up the decoder to match the target language vocabulary\n",
    "pretrained_model.config.decoder_start_token_id = tgt_tokenizer.cls_token_id\n",
    "pretrained_model.config.bos_token_id = tgt_tokenizer.cls_token_id\n",
    "pretrained_model.config.eos_token_id = tgt_tokenizer.sep_token_id\n",
    "pretrained_model.config.pad_token_id = tgt_tokenizer.pad_token_id\n",
    "\n",
    "pretrained_model.config.vocab_size = tgt_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9046ad83f84e729b26a96d33fd5bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/81986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0cf6bf83cd49d5bf2f45f18e3c9b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006cfb56633e4702aa3e3b57a83de6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22775 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 50\n",
    "max_target_length = 210\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex for ex in examples[\"Function and Parameters\"]]\n",
    "    targets = [ex for ex in examples[\"Data Segment\"]]\n",
    "    model_inputs = src_tokenizer(inputs, max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    \n",
    "    # Tokenize targets with the target tokenizer\n",
    "    labels = tgt_tokenizer(targets, max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = ezsocket_dataset_tvt.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_398161/2508772183.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"sacrebleu\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tgt_tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tgt_tokenizer.pad_token_id)\n",
    "    decoded_labels = tgt_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    print(decoded_preds)\n",
    "    print(decoded_labels)\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:620: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:640: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1068' max='1068' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1068/1068 23:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.026758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.060900</td>\n",
       "      <td>0.025523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025500</td>\n",
       "      <td>0.024776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removed shared tensor {'decoder.lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:620: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:640: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:620: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n",
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:640: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1068, training_loss=0.0419939813319217, metrics={'train_runtime': 1391.3749, 'train_samples_per_second': 49.106, 'train_steps_per_second': 0.768, 'total_flos': 4068320186880000.0, 'train_loss': 0.0419939813319217, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=pretrained_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"test\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tgt_tokenizer,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_hug",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
