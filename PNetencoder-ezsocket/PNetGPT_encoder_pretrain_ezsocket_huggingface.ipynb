{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "data_files = './data_ezsocket/merged_api_para_pcapdata_dataset20k.csv'\n",
    "# ; is the tab character in Python\n",
    "ezsocket_dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "####以下函数用于讲10进制数token化#####\n",
    "def format_decimal_as_hexadecimal(decimal_str):\n",
    "    # Convert the decimal string to an integer\n",
    "    decimal_number = int(decimal_str)\n",
    "    \n",
    "    # Convert the integer to a hexadecimal string\n",
    "    hex_str = hex(decimal_number)[2:]  # Strip the '0x' prefix\n",
    "    \n",
    "    # Ensure the length of the hex string is even\n",
    "    if len(hex_str) % 2 != 0:\n",
    "        hex_str = '0' + hex_str\n",
    "    \n",
    "    # Split the hex string into pairs of characters\n",
    "    hex_pairs = [hex_str[i:i+2] for i in range(0, len(hex_str), 2)]\n",
    "    \n",
    "    # Join the pairs with commas\n",
    "    formatted_hex = ','.join(hex_pairs)\n",
    "    \n",
    "    return formatted_hex\n",
    "\n",
    "\n",
    "def convert_number(num_str):\n",
    "    num = float(num_str)\n",
    "    if num.is_integer():\n",
    "        num = int(num)\n",
    "        sign = \"-\" if num < 0 else \"+\"\n",
    "        num_str = format_decimal_as_hexadecimal(str(num).lstrip('-'))\n",
    "        return f\"num,{sign},{num_str},num\"\n",
    "    else:\n",
    "        sign = \"-\" if num < 0 else \"+\"\n",
    "        num_str = num_str.lstrip('-')\n",
    "        integer_part, fractional_part = num_str.split('.')\n",
    "        combined_num = format_decimal_as_hexadecimal(integer_part + fractional_part.rstrip('0'))\n",
    "        pos_num = format_decimal_as_hexadecimal(len(fractional_part.rstrip('0')))\n",
    "        return f\"num,{sign},{combined_num},pos,{pos_num},num\"\n",
    "\n",
    "def process_segment(segment):\n",
    "    parts = segment.split(',')\n",
    "    for i, part in enumerate(parts):\n",
    "        if re.match(r'^-?\\d+(\\.\\d+)?$', part):  # Match integers and floating-point numbers\n",
    "            parts[i] = convert_number(part)\n",
    "    result = ','.join(parts)\n",
    "    result = result.replace(\",\", \" \")\n",
    "    return result\n",
    "####以上函数用于将10进制数token化：process_segment(segment)#####\n",
    "\n",
    "####以下函数用于将payload按两位分开，用','隔开#####\n",
    "def split_payload_into_pairs(text):\n",
    "    # 将文本按每两个字符分割\n",
    "    pairs = [text[i:i+2] for i in range(0, len(text), 2)]\n",
    "    # 用逗号连接分割后的文本\n",
    "    result = ' '.join(pairs)\n",
    "    return result\n",
    "####以上函数用于将payload按两位分开，用','隔开#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用map+lambda清洗数据\n",
    "# clear_ezsocket_dataset = ezsocket_dataset.map(lambda x: {\"Function and Parameters\": x[\"Function and Parameters\"].split(',', 1)[1]})\n",
    "clear_ezsocket_dataset = ezsocket_dataset.map(lambda x: {\"Function and Parameters\": [o.split(',', 1)[1] for o in x[\"Function and Parameters\"]]}, batched=True) #可加速处理，删除前面的时间戳\n",
    "clear_ezsocket_dataset = clear_ezsocket_dataset.map(lambda x: {\"Function and Parameters\": [process_segment(o) for o in x[\"Function and Parameters\"]]}, batched=True) #可加速处理，10进制参数token化\n",
    "clear_ezsocket_dataset = clear_ezsocket_dataset.map(lambda x: {\"Data Segment\": [split_payload_into_pairs(o) for o in x[\"Data Segment\"]]}, batched=True) #可加速处理，10进制参数token化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 14623\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 1625\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Function and Parameters', 'Data Segment'],\n",
       "        num_rows: 4063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#划分训练集测试集和验证集\n",
    "ezsocket_dataset_tt = clear_ezsocket_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "ezsocket_dataset_tvt = ezsocket_dataset_tt[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "ezsocket_dataset_tvt[\"validation\"] = ezsocket_dataset_tvt.pop(\"test\")\n",
    "ezsocket_dataset_tvt[\"test\"] = ezsocket_dataset_tt[\"test\"]\n",
    "ezsocket_dataset_tvt\n",
    "#保存数据集使用：Arrow:\tDataset.save_to_disk()  CSV:\tDataset.to_csv()    JSON:\tDataset.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "#手动tokenizer,直接生成词汇表\n",
    "word_list = set([])\n",
    "for line in ezsocket_dataset_tvt[\"train\"][\"Function and Parameters\"]:\n",
    "    line = line.split()\n",
    "    word_list.update(set(line)) # ['hello', 'how', 'are', 'you',...]\n",
    "filtered_list = [item.lower() for item in word_list if len(item) >= 6]\n",
    "\n",
    "word2idx = {f\"{i:02X}\".lower(): i for i in range(256)}\n",
    "word2idx.update({'[MASK]' : 256, '[CLS]' : 257, '[SEP]' : 258, 'num' : 259, 'pos' : 260, '[PAD]' : 261, '+' : 262, '-' : 263})\n",
    "for i, w in enumerate(filtered_list):\n",
    "    word2idx[w] = i + 264\n",
    "with open('vocab.txt', 'w', encoding='utf-8') as file:\n",
    "    for i in word2idx:\n",
    "        file.write(f\"{i}\\n\")\n",
    "\n",
    "# 加载手动生成的词汇表\n",
    "tokenizer=BertTokenizer(vocab_file='./vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "356"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23999e4315154fc7a388aa59e9e9b679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/14623 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lizedong/anaconda3/envs/py310_hug/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaf2e91b6ee40bba91d88f72fdee1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/1625 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8f61554026405c9f614341310ad47b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/4063 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Remove empty lines\n",
    "    return tokenizer(\n",
    "        examples[\"Function and Parameters\"], \n",
    "        examples[\"Data Segment\"], \n",
    "        padding=\"max_length\", # 进行填充\n",
    "        truncation=True, # 进行截断\n",
    "        max_length=210, # 设置句子的长度\n",
    "        # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n",
    "        # receives the `special_tokens_mask`.\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "tokenized_datasets = ezsocket_dataset_tvt.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=8,\n",
    "    # remove_columns=[text_column_name],\n",
    "    load_from_cache_file=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, BertTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=768,\n",
    "    num_hidden_layers=12,\n",
    "    num_attention_heads=12,\n",
    "    intermediate_size=768,\n",
    ")\n",
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(356, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=356, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_pretrained\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    # prediction_loss_only=True,\n",
    "    # no_cuda=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TrainOutput(global_step=256060, training_loss=0.09054873210884934, metrics={'train_runtime': 95449.4276, 'train_samples_per_second': 42.922, 'train_steps_per_second': 2.683, 'total_flos': 2.22759212644584e+17, 'train_loss': 0.09054873210884934, 'epoch': 10.0})\n",
    "# for obj in trainer.state.log_history:\n",
    "#     print(obj)\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# Convert the log history to a pandas DataFrame\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('training_log_history_bert_pretrain_500k.csv', index=False)\n",
    "\n",
    "print(\"Log history saved as 'training_log_history_bert_pretrain_500k.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#画图\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data from trainer.state.log_history\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "for entry in trainer.state.log_history:\n",
    "    if 'loss' in entry:\n",
    "        training_loss.append((entry['step'], entry['loss']))\n",
    "    if 'eval_loss' in entry:\n",
    "        validation_loss.append((entry['step'], entry['eval_loss']))\n",
    "\n",
    "# Separate the steps and losses\n",
    "train_steps, train_losses = zip(*training_loss)\n",
    "val_steps, val_losses = zip(*validation_loss)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_steps, train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_steps, val_losses, label='Validation Loss', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Optionally, save the plot\n",
    "plt.savefig('loss_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Extract the data from trainer.state.log_history\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "learning_rates = []\n",
    "for entry in trainer.state.log_history:\n",
    "    if 'loss' in entry:\n",
    "        training_loss.append((entry['step'], entry['loss']))\n",
    "    if 'eval_loss' in entry:\n",
    "        validation_loss.append((entry['step'], entry['eval_loss']))\n",
    "    if 'learning_rate' in entry:\n",
    "        learning_rates.append((entry['step'], entry['learning_rate']))\n",
    "\n",
    "# Separate the steps and values\n",
    "train_steps, train_losses = zip(*training_loss)\n",
    "val_steps, val_losses = zip(*validation_loss)\n",
    "lr_steps, lr_values = zip(*learning_rates)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot losses\n",
    "ax1.set_xlabel('Steps')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.plot(train_steps, train_losses, label='Training Loss', color='blue')\n",
    "ax1.plot(val_steps, val_losses, label='Validation Loss', color='red')\n",
    "ax1.tick_params(axis='y')\n",
    "\n",
    "# Create a second y-axis for learning rate\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Learning Rate')\n",
    "ax2.plot(lr_steps, lr_values, label='Learning Rate', color='green', linestyle='--')\n",
    "ax2.tick_params(axis='y')\n",
    "\n",
    "# Add title and legend\n",
    "plt.title('Training and Validation Loss with Learning Rate')\n",
    "fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
    "\n",
    "# Add grid for better readability\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Optionally, save the plot\n",
    "plt.savefig('loss_and_lr_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
